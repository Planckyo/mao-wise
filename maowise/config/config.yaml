paths:
  data_raw: datasets/data_raw
  data_parsed: datasets/data_parsed
  versions: datasets/versions
  index_store: datasets/index_store
  models_ckpt: models_ckpt/fwd_v1
  reports: reports

schema:
  file: maowise/config/schema.json

kb:
  embed_model: BAAI/bge-m3
  normalize_embeddings: true
  topk_default: 5

fwd_model:
  base_model: bert-base-multilingual-cased
  checkpoint_dir: models_ckpt/fwd_v1
  mc_dropout_passes: 5

optimize:
  bounds:
    voltage_V: [150, 700]
    current_density_A_dm2: [1.0, 40.0]
    frequency_Hz: [50, 2000]
    duty_cycle_pct: [5, 80]
    time_min: [1, 120]
    temp_C: [15, 60]
    pH: [6.0, 14.0]
  forbidden_additives: ["Cr6+"]
  nsga2_pop_size: 48
  nsga2_n_gen: 30
  refinement_budget: 32
  # 多目标权重配置
  weights:
    alpha: 0.4        # Alpha 性能权重
    epsilon: 0.4      # Epsilon 性能权重
    thin_light: 0.15  # 薄/轻目标权重
    uniform: 0.05     # 均匀性目标权重
  # 薄/轻目标阈值
  thin_light_targets:
    mass_proxy_threshold: 0.4      # 质量代理值阈值
    uniformity_threshold: 0.2      # 均匀性惩罚阈值
  # 体系特定的均匀性窗口（可选覆盖）
  uniformity_windows:
    silicate:
      duty_cycle: [15, 30]    # 占空比推荐范围 (%)
      frequency: [800, 1200]  # 频率推荐范围 (Hz)
    zirconate:
      duty_cycle: [20, 40] 
      frequency: [600, 1000]
    phosphate:
      duty_cycle: [10, 25]
      frequency: [1000, 1500]

thresholds:
  delta_alpha: 0.03
  delta_epsilon: 0.03

runtime:
  use_cuda_if_available: true

library_dir: ${MAOWISE_LIBRARY_DIR}

llm:
  provider: ${LLM_PROVIDER:-local}
  openai:
    api_key: ${OPENAI_API_KEY}
    base_url: ${OPENAI_BASE_URL}
    model: ${OPENAI_MODEL:-gpt-4o-mini}
  timeout_s: ${LLM_TIMEOUT_S:-60}
  max_tokens: ${LLM_MAX_TOKENS:-1024}
  temperature: ${LLM_TEMPERATURE:-0.2}
  offline_fallback: true
  cache_dir: datasets/cache
  rate_limit:
    requests_per_minute: 60
  limits:
    max_concurrent_requests: ${LLM_MAX_CONCURRENT:-5}
    max_requests_per_minute: ${LLM_MAX_RPM:-100}
    max_tokens_per_minute: ${LLM_MAX_TPM:-50000}
    cost_limit_per_day_usd: ${LLM_COST_LIMIT:-10.0}
  usage_tracking:
    enabled: true
    log_file: datasets/cache/llm_usage.csv
    log_tokens: true
    log_costs: true
  debug:
    print_full_prompts: ${DEBUG_LLM:-false}

