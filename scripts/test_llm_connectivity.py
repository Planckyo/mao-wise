#!/usr/bin/env python3
"""
LLM è¿æ¥å¥åº·æ£€æŸ¥è„šæœ¬
"""

import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from maowise.llm.client import llm_chat
from maowise.utils import load_config
from maowise.utils.logger import logger


def test_basic_chat():
    """æµ‹è¯•åŸºæœ¬èŠå¤©åŠŸèƒ½"""
    messages = [
        {"role": "user", "content": "Hello, this is a connectivity test. Please respond with 'OK'."}
    ]
    
    try:
        response = llm_chat(messages)
        content = response.get("content", "")
        usage = response.get("usage", {})
        
        print(f"âœ… LLM Response: {content[:100]}...")
        print(f"ğŸ“Š Token Usage: {usage}")
        
        return True
        
    except Exception as e:
        print(f"âŒ LLM Error: {e}")
        return False


def test_json_output():
    """æµ‹è¯• JSON æ ¼å¼è¾“å‡º"""
    messages = [
        {
            "role": "user", 
            "content": "Return a JSON object with keys 'status' and 'message'. Set status to 'ok' and message to 'test successful'."
        }
    ]
    
    try:
        response = llm_chat(
            messages,
            response_format={"type": "json_object"}
        )
        
        content = response.get("content", "")
        print(f"âœ… JSON Response: {content}")
        
        # Try to parse JSON
        import json
        json.loads(content)
        print("âœ… Valid JSON format")
        
        return True
        
    except Exception as e:
        print(f"âŒ JSON Test Error: {e}")
        return False


def test_rag_integration():
    """æµ‹è¯• RAG é›†æˆ"""
    try:
        from maowise.llm.rag import build_context
        
        context = build_context("micro-arc oxidation", topk=3)
        print(f"âœ… RAG Context: Found {len(context)} snippets")
        
        if context:
            for i, snippet in enumerate(context[:2], 1):
                print(f"   Snippet {i}: {snippet.text[:50]}... (score: {snippet.score:.3f})")
        
        return True
        
    except Exception as e:
        print(f"âŒ RAG Test Error: {e}")
        return False


def test_cache_functionality():
    """æµ‹è¯•ç¼“å­˜åŠŸèƒ½"""
    messages = [
        {"role": "user", "content": "Cache test message - respond with current timestamp if possible"}
    ]
    
    try:
        # First call
        import time
        start_time = time.time()
        response1 = llm_chat(messages, use_cache=True)
        first_call_time = time.time() - start_time
        
        # Second call (should be cached)
        start_time = time.time()
        response2 = llm_chat(messages, use_cache=True)
        second_call_time = time.time() - start_time
        
        print(f"âœ… Cache Test:")
        print(f"   First call: {first_call_time:.3f}s")
        print(f"   Second call: {second_call_time:.3f}s")
        
        if second_call_time < first_call_time * 0.5:
            print("âœ… Cache appears to be working (faster second call)")
        else:
            print("âš ï¸  Cache may not be working (similar call times)")
        
        return True
        
    except Exception as e:
        print(f"âŒ Cache Test Error: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” MAO-Wise LLM è¿æ¥å¥åº·æ£€æŸ¥")
    print("=" * 50)
    
    # åŠ è½½é…ç½®
    try:
        config = load_config()
        llm_config = config.get("llm", {})
        provider = llm_config.get("provider", "local")
        
        print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
        print(f"   Provider: {provider}")
        print(f"   Offline Fallback: {llm_config.get('offline_fallback', True)}")
        print(f"   Cache Directory: {llm_config.get('cache_dir', 'datasets/cache')}")
        print()
        
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # æ£€æŸ¥ API Key (ä»…å¯¹ openai/azure)
    if provider in ["openai", "azure"]:
        import os
        if provider == "openai":
            api_key = llm_config.get("openai", {}).get("api_key") or os.getenv("OPENAI_API_KEY")
            if not api_key:
                print("âš ï¸  æœªè®¾ç½® OPENAI_API_KEYï¼Œå°†ä½¿ç”¨ç¦»çº¿å…œåº•æ¨¡å¼")
        elif provider == "azure":
            api_key = os.getenv("AZURE_OPENAI_API_KEY")
            if not api_key:
                print("âš ï¸  æœªè®¾ç½® AZURE_OPENAI_API_KEYï¼Œå°†ä½¿ç”¨ç¦»çº¿å…œåº•æ¨¡å¼")
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        ("åŸºæœ¬èŠå¤©æµ‹è¯•", test_basic_chat),
        ("JSON è¾“å‡ºæµ‹è¯•", test_json_output),
        ("RAG é›†æˆæµ‹è¯•", test_rag_integration),
        ("ç¼“å­˜åŠŸèƒ½æµ‹è¯•", test_cache_functionality),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"ğŸ§ª {test_name}...")
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name} é€šè¿‡\n")
            else:
                print(f"âŒ {test_name} å¤±è´¥\n")
        except Exception as e:
            print(f"âŒ {test_name} å¼‚å¸¸: {e}\n")
    
    # æ€»ç»“
    print("=" * 50)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LLM ç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚")
        return 0
    elif passed > 0:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•é€šè¿‡ã€‚è¯·æ£€æŸ¥é…ç½®æˆ–ç½‘ç»œè¿æ¥ã€‚")
        if provider == "local":
            print("ğŸ’¡ æç¤º: å½“å‰ä½¿ç”¨æœ¬åœ°æ¨¡å¼ï¼Œè¿™æ˜¯æ­£å¸¸çš„ç¦»çº¿å…œåº•è¡Œä¸ºã€‚")
        return 0  # éƒ¨åˆ†é€šè¿‡ä¹Ÿç®—æˆåŠŸï¼Œå› ä¸ºæœ‰ç¦»çº¿å…œåº•
    else:
        print("âŒ æ‰€æœ‰æµ‹è¯•å¤±è´¥ã€‚è¯·æ£€æŸ¥é…ç½®ã€‚")
        return 1


if __name__ == "__main__":
    sys.exit(main())
