#!/usr/bin/env python3
"""
é¢„æµ‹è¯„ä¼°è„šæœ¬

å¯¹å®éªŒæ•°æ®è¿›è¡Œé¢„æµ‹è¯„ä¼°ï¼Œç”Ÿæˆè¯¦ç»†æŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨ã€‚

åŠŸèƒ½ç‰¹æ€§ï¼š
- è¯»å–experiments.parquetå®éªŒæ•°æ®
- è°ƒç”¨/predict APIæˆ–æœ¬åœ°æ¨ç†ç®¡çº¿ç”Ÿæˆé¢„æµ‹
- è®¡ç®—å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼šMAE/MAPE/RMSEã€å‘½ä¸­ç‡ã€ç½®ä¿¡åº¦åˆ†æ
- ç”ŸæˆJSONæŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨
- æŒ‰ä½“ç³»åˆ†ç»„çš„è¯¦ç»†åˆ†æ
- æ”¯æŒå¹²è¿è¡Œæ¨¡å¼ç”¨äºæ¨¡å‹å¯¹æ¯”

ä½¿ç”¨ç¤ºä¾‹ï¼š
python scripts/evaluate_predictions.py
python scripts/evaluate_predictions.py --dry-run --output reports/eval_before_update.json
python scripts/evaluate_predictions.py --api-url http://localhost:8000
"""

import argparse
import json
import sys
import pathlib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import requests
import logging

# ç¡®ä¿èƒ½æ‰¾åˆ°maowiseåŒ…
REPO_ROOT = pathlib.Path(__file__).resolve().parent.parent
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from maowise.utils.logger import logger

class PredictionEvaluator:
    """é¢„æµ‹è¯„ä¼°å™¨"""
    
    def __init__(self, experiments_file: str = "datasets/samples.parquet", 
                 api_url: str = "http://localhost:8000",
                 split: str = "all"):
        self.experiments_file = pathlib.Path(experiments_file)
        self.api_url = api_url.rstrip('/')
        self.reports_dir = pathlib.Path("reports")
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        self.split = split
        
        # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
    def _load_experiment_data(self) -> pd.DataFrame:
        """åŠ è½½å®éªŒæ•°æ®å¹¶æŒ‰splitè¿‡æ»¤"""
        if not self.experiments_file.exists():
            raise FileNotFoundError(f"å®éªŒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.experiments_file}")
        
        try:
            df = pd.read_parquet(self.experiments_file)
            logger.info(f"åŠ è½½å®éªŒæ•°æ®: {len(df)} æ¡è®°å½•")
            
            # æŒ‰splitè¿‡æ»¤æ•°æ®
            if self.split != "all" and "split" in df.columns:
                df_split = df[df['split'] == self.split].copy()
                logger.info(f"æŒ‰split='{self.split}'è¿‡æ»¤: {len(df_split)} æ¡è®°å½•")
            else:
                df_split = df.copy()
                if self.split != "all" and "split" not in df.columns:
                    logger.warning(f"æ•°æ®ä¸­æ— 'split'åˆ—ï¼Œå¿½ç•¥splitå‚æ•°ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®")
            
            # éªŒè¯å¿…éœ€å­—æ®µ
            required_fields = ['measured_alpha', 'measured_epsilon']
            missing_fields = [f for f in required_fields if f not in df_split.columns]
            if missing_fields:
                raise ValueError(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")
            
            # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
            valid_mask = (
                df_split['measured_alpha'].notna() & 
                df_split['measured_epsilon'].notna() &
                (df_split['measured_alpha'] >= 0) & (df_split['measured_alpha'] <= 1) &
                (df_split['measured_epsilon'] >= 0) & (df_split['measured_epsilon'] <= 2)
            )
            
            df_valid = df_split[valid_mask].copy()
            logger.info(f"æœ‰æ•ˆæ•°æ®: {len(df_valid)} æ¡è®°å½•")
            
            if len(df_valid) == 0:
                raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„å®éªŒæ•°æ®")
            
            return df_valid
            
        except Exception as e:
            raise ValueError(f"åŠ è½½å®éªŒæ•°æ®å¤±è´¥: {e}")
    
    def _prepare_prediction_input(self, row: pd.Series) -> Dict[str, Any]:
        """å‡†å¤‡é¢„æµ‹è¾“å…¥"""
        # æ„é€ é¢„æµ‹è¾“å…¥ï¼Œä¼˜å…ˆä½¿ç”¨å®éªŒå‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼
        input_data = {
            "substrate_alloy": row.get('substrate_alloy', 'AZ91D'),
            "electrolyte_family": self._infer_electrolyte_family(row.get('system', 'mixed')),
            "electrolyte_components": self._parse_electrolyte_components(row.get('electrolyte_components_json', '[]')),
            "mode": "ac",  # å‡è®¾å¤§å¤šæ•°æ˜¯äº¤æµæ¨¡å¼
            "voltage_V": float(row.get('voltage_V', 300.0)),
            "current_density_A_dm2": float(row.get('current_density_Adm2', row.get('current_density_A_dm2', 10.0))),
            "frequency_Hz": float(row.get('frequency_Hz', 1000.0)),
            "duty_cycle_pct": float(row.get('duty_cycle_pct', 30.0)),
            "time_min": float(row.get('time_min', 20.0)),
            "temp_C": float(row.get('temp_C', 25.0)) if pd.notna(row.get('temp_C')) else 25.0,
            "pH": float(row.get('pH', 11.0)) if pd.notna(row.get('pH')) else 11.0,
            "sealing": row.get('post_treatment', 'none') if row.get('post_treatment') != 'none' else 'none'
        }
        
        return input_data
    
    def _infer_electrolyte_family(self, system: str) -> str:
        """æ ¹æ®ä½“ç³»æ¨æ–­ç”µè§£æ¶²æ—"""
        system = str(system).lower()
        if 'silicate' in system:
            return 'alkaline'
        elif 'zirconate' in system:
            return 'fluoride'
        else:
            return 'mixed'
    
    def _parse_electrolyte_components(self, components_json: str) -> List[str]:
        """è§£æç”µè§£æ¶²ç»„åˆ†"""
        try:
            if pd.isna(components_json) or components_json == '':
                return []
            return json.loads(components_json)
        except:
            return []
    
    def _predict_via_api(self, input_data: Dict[str, Any]) -> Dict[str, float]:
        """é€šè¿‡APIè¿›è¡Œé¢„æµ‹"""
        try:
            response = requests.post(
                f"{self.api_url}/api/maowise/v1/predict",
                json=input_data,
                timeout=30
            )
            response.raise_for_status()
            result = response.json()
            
            return {
                'pred_alpha': result.get('alpha_150_2600', 0.0),
                'pred_epsilon': result.get('epsilon_3000_30000', 0.0),
                'confidence': result.get('confidence', 0.5)
            }
        except Exception as e:
            logger.warning(f"APIé¢„æµ‹å¤±è´¥: {e}")
            return self._predict_local_fallback(input_data)
    
    def _predict_local_fallback(self, input_data: Dict[str, Any]) -> Dict[str, float]:
        """æœ¬åœ°é¢„æµ‹é™çº§æ–¹æ¡ˆ"""
        try:
            # å°è¯•å¯¼å…¥æœ¬åœ°æ¨ç†æ¨¡å—
            from maowise.models.infer_fwd import predict_properties
            result = predict_properties(input_data)
            
            return {
                'pred_alpha': result.get('alpha_150_2600', 0.0),
                'pred_epsilon': result.get('epsilon_3000_30000', 0.0),
                'confidence': result.get('confidence', 0.5)
            }
        except Exception as e:
            logger.warning(f"æœ¬åœ°é¢„æµ‹ä¹Ÿå¤±è´¥: {e}")
            # æœ€ç»ˆé™çº§ï¼šåŸºäºç»éªŒçš„ç®€å•é¢„æµ‹
            return self._simple_baseline_prediction(input_data)
    
    def _simple_baseline_prediction(self, input_data: Dict[str, Any]) -> Dict[str, float]:
        """ç®€å•åŸºçº¿é¢„æµ‹ï¼ˆåŸºäºç»éªŒè§„å¾‹ï¼‰"""
        # åŸºäºç”µå‹å’Œç”µæµå¯†åº¦çš„ç®€å•ç»éªŒå…¬å¼
        voltage = input_data.get('voltage_V', 300)
        current = input_data.get('current_density_A_dm2', 10)
        
        # ç®€åŒ–çš„ç»éªŒå…¬å¼
        pred_alpha = 0.15 + (voltage - 200) * 0.0001 + (current - 5) * 0.005
        pred_epsilon = 0.7 + (voltage - 200) * 0.0003 + (current - 5) * 0.01
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        pred_alpha = np.clip(pred_alpha, 0.05, 0.4)
        pred_epsilon = np.clip(pred_epsilon, 0.5, 1.2)
        
        return {
            'pred_alpha': pred_alpha,
            'pred_epsilon': pred_epsilon,
            'confidence': 0.3  # ä½ç½®ä¿¡åº¦
        }
    
    def _calculate_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        # åŸºæœ¬å›å½’æŒ‡æ ‡
        alpha_mae = np.mean(np.abs(df['measured_alpha'] - df['pred_alpha']))
        alpha_mape = np.mean(np.abs((df['measured_alpha'] - df['pred_alpha']) / df['measured_alpha'])) * 100
        alpha_rmse = np.sqrt(np.mean((df['measured_alpha'] - df['pred_alpha']) ** 2))
        
        epsilon_mae = np.mean(np.abs(df['measured_epsilon'] - df['pred_epsilon']))
        epsilon_mape = np.mean(np.abs((df['measured_epsilon'] - df['pred_epsilon']) / df['measured_epsilon'])) * 100
        epsilon_rmse = np.sqrt(np.mean((df['measured_epsilon'] - df['pred_epsilon']) ** 2))
        
        # å‘½ä¸­ç‡æŒ‡æ ‡
        alpha_hit_003 = np.mean(np.abs(df['measured_alpha'] - df['pred_alpha']) <= 0.03) * 100
        alpha_hit_005 = np.mean(np.abs(df['measured_alpha'] - df['pred_alpha']) <= 0.05) * 100
        
        epsilon_hit_003 = np.mean(np.abs(df['measured_epsilon'] - df['pred_epsilon']) <= 0.03) * 100
        epsilon_hit_005 = np.mean(np.abs(df['measured_epsilon'] - df['pred_epsilon']) <= 0.05) * 100
        
        # ç½®ä¿¡åº¦åˆ†æ
        low_confidence_ratio = np.mean(df['confidence'] < 0.5) * 100
        avg_confidence = np.mean(df['confidence'])
        
        # ç›¸å…³æ€§
        alpha_corr = np.corrcoef(df['measured_alpha'], df['pred_alpha'])[0, 1]
        epsilon_corr = np.corrcoef(df['measured_epsilon'], df['pred_epsilon'])[0, 1]
        
        # è¿”å›æ ‡å‡†é”®åæ ¼å¼ï¼Œä¿æŒå‘åå…¼å®¹
        result = {
            # ===== æ ‡å‡†é”®å (æ–°æ ¼å¼) =====
            'alpha_mae': float(alpha_mae),
            'epsilon_mae': float(epsilon_mae),
            'alpha_rmse': float(alpha_rmse),
            'epsilon_rmse': float(epsilon_rmse),
            'alpha_hit_pm_0.03': float(alpha_hit_003),
            'epsilon_hit_pm_0.03': float(epsilon_hit_003),
            'alpha_hit_pm_0.05': float(alpha_hit_005),
            'epsilon_hit_pm_0.05': float(epsilon_hit_005),
            'confidence_mean': float(avg_confidence),
            'confidence_low_ratio': float(low_confidence_ratio),
            'sample_size': len(df),
            
            # ===== å‘åå…¼å®¹ (æ—§æ ¼å¼) =====
            'alpha_metrics': {
                'mae': float(alpha_mae),
                'mape': float(alpha_mape),
                'rmse': float(alpha_rmse),
                'hit_rate_003': float(alpha_hit_003),
                'hit_rate_005': float(alpha_hit_005),
                'correlation': float(alpha_corr)
            },
            'epsilon_metrics': {
                'mae': float(epsilon_mae),
                'mape': float(epsilon_mape),
                'rmse': float(epsilon_rmse),
                'hit_rate_003': float(epsilon_hit_003),
                'hit_rate_005': float(epsilon_hit_005),
                'correlation': float(epsilon_corr)
            },
            'confidence_metrics': {
                'average': float(avg_confidence),
                'low_confidence_ratio': float(low_confidence_ratio)
            }
        }
        
        return result
    
    def _normalize_legacy_json(self, file_path: pathlib.Path) -> bool:
        """è§„èŒƒåŒ–å†å²JSONæ–‡ä»¶çš„é”®å"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è§„èŒƒåŒ–
            needs_update = False
            
            # é€’å½’è§„èŒƒåŒ–å‡½æ•°
            def normalize_metrics(metrics_dict):
                nonlocal needs_update
                if not isinstance(metrics_dict, dict):
                    return metrics_dict
                
                # å¦‚æœå·²ç»æœ‰æ ‡å‡†é”®åï¼Œè·³è¿‡
                if 'alpha_mae' in metrics_dict:
                    return metrics_dict
                
                # æå–æ—§æ ¼å¼çš„å€¼
                alpha_metrics = metrics_dict.get('alpha_metrics', {})
                epsilon_metrics = metrics_dict.get('epsilon_metrics', {})
                confidence_metrics = metrics_dict.get('confidence_metrics', {})
                
                if alpha_metrics or epsilon_metrics or confidence_metrics:
                    needs_update = True
                    
                    # æ·»åŠ æ ‡å‡†é”®å
                    metrics_dict.update({
                        'alpha_mae': alpha_metrics.get('mae', 0.0),
                        'epsilon_mae': epsilon_metrics.get('mae', 0.0),
                        'alpha_rmse': alpha_metrics.get('rmse', 0.0),
                        'epsilon_rmse': epsilon_metrics.get('rmse', 0.0),
                        'alpha_hit_pm_0.03': alpha_metrics.get('hit_rate_003', 0.0),
                        'epsilon_hit_pm_0.03': epsilon_metrics.get('hit_rate_003', 0.0),
                        'alpha_hit_pm_0.05': alpha_metrics.get('hit_rate_005', 0.0),
                        'epsilon_hit_pm_0.05': epsilon_metrics.get('hit_rate_005', 0.0),
                        'confidence_mean': confidence_metrics.get('average', 0.0),
                        'confidence_low_ratio': confidence_metrics.get('low_confidence_ratio', 0.0)
                    })
                
                return metrics_dict
            
            # è§„èŒƒåŒ–æ•´ä½“æŒ‡æ ‡
            if 'overall_metrics' in data:
                data['overall_metrics'] = normalize_metrics(data['overall_metrics'])
            
            # è§„èŒƒåŒ–ä½“ç³»æŒ‡æ ‡
            if 'system_metrics' in data:
                for system, metrics in data['system_metrics'].items():
                    data['system_metrics'][system] = normalize_metrics(metrics)
            
            # å¦‚æœéœ€è¦æ›´æ–°ï¼Œå†™å›æ–‡ä»¶
            if needs_update:
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)
                logger.info(f"è§„èŒƒåŒ–å†å²JSONæ–‡ä»¶: {file_path}")
                return True
            
            return False
            
        except Exception as e:
            logger.warning(f"è§„èŒƒåŒ–JSONæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return False
    
    def _generate_plots(self, df: pd.DataFrame, output_prefix: str) -> List[str]:
        """ç”Ÿæˆè¯„ä¼°å›¾è¡¨"""
        plot_files = []
        
        # å›¾1: Pred vs True æ•£ç‚¹å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # Alphaæ•£ç‚¹å›¾
        ax1.scatter(df['measured_alpha'], df['pred_alpha'], alpha=0.6, c=df['confidence'], 
                   cmap='viridis', s=50)
        ax1.plot([df['measured_alpha'].min(), df['measured_alpha'].max()], 
                [df['measured_alpha'].min(), df['measured_alpha'].max()], 
                'r--', alpha=0.8, label='Perfect Prediction')
        ax1.set_xlabel('å®æµ‹ Alpha')
        ax1.set_ylabel('é¢„æµ‹ Alpha')
        ax1.set_title('Alpha é¢„æµ‹ vs å®æµ‹')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Epsilonæ•£ç‚¹å›¾
        scatter = ax2.scatter(df['measured_epsilon'], df['pred_epsilon'], alpha=0.6, 
                             c=df['confidence'], cmap='viridis', s=50)
        ax2.plot([df['measured_epsilon'].min(), df['measured_epsilon'].max()], 
                [df['measured_epsilon'].min(), df['measured_epsilon'].max()], 
                'r--', alpha=0.8, label='Perfect Prediction')
        ax2.set_xlabel('å®æµ‹ Epsilon')
        ax2.set_ylabel('é¢„æµ‹ Epsilon')
        ax2.set_title('Epsilon é¢„æµ‹ vs å®æµ‹')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ é¢œè‰²æ¡
        plt.colorbar(scatter, ax=ax2, label='ç½®ä¿¡åº¦')
        
        plt.tight_layout()
        pred_vs_true_file = self.reports_dir / f"{output_prefix}_pred_vs_true.png"
        plt.savefig(pred_vs_true_file, dpi=300, bbox_inches='tight')
        plt.close()
        plot_files.append(str(pred_vs_true_file))
        
        # å›¾2: è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # Alphaè¯¯å·®åˆ†å¸ƒ
        alpha_errors = df['measured_alpha'] - df['pred_alpha']
        ax1.hist(alpha_errors, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        ax1.axvline(0, color='red', linestyle='--', alpha=0.8, label='é›¶è¯¯å·®')
        ax1.axvline(alpha_errors.mean(), color='orange', linestyle='-', alpha=0.8, 
                   label=f'å¹³å‡è¯¯å·®: {alpha_errors.mean():.4f}')
        ax1.set_xlabel('é¢„æµ‹è¯¯å·® (å®æµ‹ - é¢„æµ‹)')
        ax1.set_ylabel('é¢‘æ¬¡')
        ax1.set_title('Alpha é¢„æµ‹è¯¯å·®åˆ†å¸ƒ')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Epsilonè¯¯å·®åˆ†å¸ƒ
        epsilon_errors = df['measured_epsilon'] - df['pred_epsilon']
        ax2.hist(epsilon_errors, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')
        ax2.axvline(0, color='red', linestyle='--', alpha=0.8, label='é›¶è¯¯å·®')
        ax2.axvline(epsilon_errors.mean(), color='orange', linestyle='-', alpha=0.8, 
                   label=f'å¹³å‡è¯¯å·®: {epsilon_errors.mean():.4f}')
        ax2.set_xlabel('é¢„æµ‹è¯¯å·® (å®æµ‹ - é¢„æµ‹)')
        ax2.set_ylabel('é¢‘æ¬¡')
        ax2.set_title('Epsilon é¢„æµ‹è¯¯å·®åˆ†å¸ƒ')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        error_dist_file = self.reports_dir / f"{output_prefix}_error_distribution.png"
        plt.savefig(error_dist_file, dpi=300, bbox_inches='tight')
        plt.close()
        plot_files.append(str(error_dist_file))
        
        return plot_files
    
    def evaluate(self, dry_run: bool = False, output_file: Optional[str] = None) -> Dict[str, Any]:
        """æ‰§è¡Œè¯„ä¼°"""
        # åŠ è½½å®éªŒæ•°æ®
        df = self._load_experiment_data()
        
        if dry_run:
            logger.info("DRY RUN - ä½¿ç”¨ç°æœ‰é¢„æµ‹ç»“æœæˆ–è·³è¿‡é¢„æµ‹")
        else:
            logger.info("å¼€å§‹ç”Ÿæˆé¢„æµ‹...")
        
        # ç”Ÿæˆé¢„æµ‹
        predictions = []
        for idx, row in df.iterrows():
            if dry_run and all(col in df.columns for col in ['pred_alpha', 'pred_epsilon', 'confidence']):
                # å¹²è¿è¡Œä¸”å·²æœ‰é¢„æµ‹ç»“æœ
                pred = {
                    'pred_alpha': row['pred_alpha'],
                    'pred_epsilon': row['pred_epsilon'], 
                    'confidence': row['confidence']
                }
            else:
                # ç”Ÿæˆæ–°é¢„æµ‹
                input_data = self._prepare_prediction_input(row)
                pred = self._predict_via_api(input_data)
            
            predictions.append(pred)
            
            if (idx + 1) % 10 == 0:
                logger.info(f"å·²å¤„ç† {idx + 1}/{len(df)} æ¡è®°å½•")
        
        # æ·»åŠ é¢„æµ‹ç»“æœåˆ°DataFrame
        pred_data = {
            'pred_alpha': [pred['pred_alpha'] for pred in predictions],
            'pred_epsilon': [pred['pred_epsilon'] for pred in predictions],
            'confidence': [pred['confidence'] for pred in predictions]
        }
        
        for col, values in pred_data.items():
            df[col] = values
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        overall_metrics = self._calculate_metrics(df)
        
        # æŒ‰ä½“ç³»åˆ†ç»„è®¡ç®—æŒ‡æ ‡
        system_metrics = {}
        if 'system' in df.columns:
            for system in df['system'].unique():
                if pd.notna(system):
                    system_df = df[df['system'] == system]
                    if len(system_df) > 0:
                        system_metrics[system] = self._calculate_metrics(system_df)
        
        # ç”Ÿæˆå›¾è¡¨
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_prefix = f"eval_experiments_{timestamp}"
        if output_file:
            output_prefix = pathlib.Path(output_file).stem
        
        plot_files = self._generate_plots(df, output_prefix)
        
        # è®¡ç®—ç›®æ ‡è¾¾æˆæƒ…å†µ
        target_achieved = {
            'epsilon_mae_le_006': float(overall_metrics.get('epsilon_mae', 1.0) <= 0.006),
            'alpha_mae_le_003': float(overall_metrics.get('alpha_mae', 1.0) <= 0.003),
            'epsilon_hit_pm_003_ge_90': float(overall_metrics.get('epsilon_hit_pm_0.03', 0.0) >= 90.0),
            'alpha_hit_pm_003_ge_90': float(overall_metrics.get('alpha_hit_pm_0.03', 0.0) >= 90.0),
            'confidence_mean_ge_07': float(overall_metrics.get('confidence_mean', 0.0) >= 0.7)
        }
        
        # æ„å»ºè¯„ä¼°ç»“æœ
        result = {
            'evaluation_time': datetime.now().isoformat(),
            'data_info': {
                'total_records': len(df),
                'experiment_file': str(self.experiments_file),
                'split': self.split,
                'systems': df['system'].value_counts().to_dict() if 'system' in df.columns else {}
            },
            'overall_metrics': overall_metrics,
            'system_metrics': system_metrics,
            'target_achieved': target_achieved,
            'plots': plot_files,
            'dry_run': dry_run
        }
        
        # ä¿å­˜ç»“æœ
        if not output_file:
            output_file = self.reports_dir / f"{output_prefix}.json"
        else:
            output_file = pathlib.Path(output_file)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        
        # è§„èŒƒåŒ–å†å²JSONæ–‡ä»¶çš„é”®å
        reports_pattern = self.reports_dir / "eval_experiments_*.json"
        import glob
        for json_file in glob.glob(str(reports_pattern)):
            json_path = pathlib.Path(json_file)
            if json_path != output_file:  # ä¸å¤„ç†å½“å‰åˆšç”Ÿæˆçš„æ–‡ä»¶
                self._normalize_legacy_json(json_path)
        
        return result

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="é¢„æµ‹è¯„ä¼°è„šæœ¬ - è¯„ä¼°æ¨¡å‹é¢„æµ‹æ€§èƒ½",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # æ ‡å‡†è¯„ä¼°
  python scripts/evaluate_predictions.py
  
  # å¹²è¿è¡Œæ¨¡å¼ï¼ˆç”¨äºæ¨¡å‹å¯¹æ¯”ï¼‰
  python scripts/evaluate_predictions.py --dry-run --output reports/eval_before_update.json
  
  # æŒ‡å®šAPIåœ°å€
  python scripts/evaluate_predictions.py --api-url http://localhost:8000
        """
    )
    
    parser.add_argument("--experiments-file", 
                       type=str, 
                       default="datasets/samples.parquet",
                       help="å®éªŒæ•°æ®æ–‡ä»¶è·¯å¾„")
    
    parser.add_argument("--api-url", 
                       type=str,
                       default="http://localhost:8000",
                       help="APIæœåŠ¡åœ°å€")
    
    parser.add_argument("--output", 
                       type=str,
                       help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    parser.add_argument("--dry-run", 
                       action="store_true",
                       help="å¹²è¿è¡Œæ¨¡å¼ï¼Œä½¿ç”¨ç°æœ‰é¢„æµ‹æˆ–è·³è¿‡é¢„æµ‹")
    
    parser.add_argument("--split", 
                       choices=["val", "test", "all"],
                       default="all",
                       help="æ•°æ®é›†åˆ†å‰²é€‰æ‹© (é»˜è®¤: all)")
    
    args = parser.parse_args()
    
    try:
        evaluator = PredictionEvaluator(
            experiments_file=args.experiments_file,
            api_url=args.api_url,
            split=args.split
        )
        
        print("ğŸ” å¼€å§‹é¢„æµ‹è¯„ä¼°...")
        print(f"   å®éªŒæ•°æ®: {args.experiments_file}")
        print(f"   APIåœ°å€: {args.api_url}")
        if args.dry_run:
            print("   æ¨¡å¼: å¹²è¿è¡Œ")
        
        result = evaluator.evaluate(dry_run=args.dry_run, output_file=args.output)
        
        # æ‰“å°æ‘˜è¦
        print(f"\nğŸ“Š è¯„ä¼°å®Œæˆ!")
        print(f"   - æ•°æ®è®°å½•: {result['data_info']['total_records']}")
        print(f"   - æŠ¥å‘Šæ–‡ä»¶: {args.output or 'reports/eval_experiments_*.json'}")
        print(f"   - å›¾è¡¨æ–‡ä»¶: {len(result['plots'])} å¼ ")
        
        # æ‰“å°å…³é”®æŒ‡æ ‡
        overall = result['overall_metrics']
        print(f"\nğŸ“ˆ æ€»ä½“æ€§èƒ½:")
        print(f"   Alpha MAE: {overall['alpha_metrics']['mae']:.4f}")
        print(f"   Alpha å‘½ä¸­ç‡(Â±0.03): {overall['alpha_metrics']['hit_rate_003']:.1f}%")
        print(f"   Epsilon MAE: {overall['epsilon_metrics']['mae']:.4f}")
        print(f"   Epsilon å‘½ä¸­ç‡(Â±0.03): {overall['epsilon_metrics']['hit_rate_003']:.1f}%")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {overall['confidence_metrics']['average']:.3f}")
        print(f"   ä½ç½®ä¿¡åº¦æ¯”ä¾‹: {overall['confidence_metrics']['low_confidence_ratio']:.1f}%")
        
        # æŒ‰ä½“ç³»æ‰“å°
        if result['system_metrics']:
            print(f"\nğŸ”¬ åˆ†ä½“ç³»æ€§èƒ½:")
            for system, metrics in result['system_metrics'].items():
                print(f"   {system}:")
                print(f"     Alpha MAE: {metrics['alpha_metrics']['mae']:.4f}")
                print(f"     Alpha å‘½ä¸­ç‡(Â±0.03): {metrics['alpha_metrics']['hit_rate_003']:.1f}%")
                print(f"     Epsilon MAE: {metrics['epsilon_metrics']['mae']:.4f}")
                print(f"     Epsilon å‘½ä¸­ç‡(Â±0.03): {metrics['epsilon_metrics']['hit_rate_003']:.1f}%")
                print(f"     æ ·æœ¬æ•°: {metrics['sample_size']}")
        
    except Exception as e:
        logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
        print(f"âŒ é”™è¯¯: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
